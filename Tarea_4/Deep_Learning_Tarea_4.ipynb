{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73c2f97b-36c1-4359-83d8-a6c6f7a6f412",
      "metadata": {
        "id": "73c2f97b-36c1-4359-83d8-a6c6f7a6f412"
      },
      "source": [
        "# Tarea 4\n",
        "\n",
        "## Estudiante: Andrés Calderón Guardia\n",
        "\n",
        "Usamos un dataset de analisis de sentimiento multimodal **centrada en entidades** en tweets. Cada tweet viene con una etiqueta entre `positive`, `negative` y `neutral`.\n",
        "\n",
        "Los tweets vienen de este formato textual, y cada tweet tiene una imagen:\n",
        "\n",
        "* `Harriette </s> $T$ moved back to Chicago to care for her mom : And it ' s been terrible # NextDayChi 17` tiene la etiqueta `neutral` y la imagen `twitter2015_images/71274.jpg`\n",
        "* `Chicago </s> Harriette moved back to $T$ to care for her mom : And it ' s been terrible # NextDayChi 17` tiene la etiqueta `negative` y la imagen `twitter2015_images/71274.jpg`\n",
        "\n",
        "**Vamos a ver como usar:**\n",
        "* Fusionar datos de imagenes y de texto para una tarea de clasificacion\n",
        "* Utilizar las representaciones de las imagenes modalidades tal que (pesos de los encoders congelados)\n",
        "* Fine-tunear todo el network\n",
        "* Tratar varios metodos de fusion con capa de attencion\n",
        "* Utilizar un modelo grande pre-entrenado de manera multimodal como VL-BERT or ViLT\n",
        "* Utilizar un LMM tipo BLIP3 pre-entrenado con instrucciones, usando Zero-shot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cae5356-8d8d-437a-86cf-dd294a8d0f47",
      "metadata": {
        "id": "0cae5356-8d8d-437a-86cf-dd294a8d0f47"
      },
      "source": [
        "## Se puede utilizar LLM para ayudarse!\n",
        "Mirar a [un ejemplo](https://chatgpt.com/share/d534833e-bd2c-40c1-81eb-34818b195cac) de como pedir las respuestas a un LLM. Mejor si entenden lo que hagan.\n",
        "\n",
        "#### Para cada entrenamiento\n",
        "* Informa de las curvas de pérdida y precisión en el entrenamiento y la validación.\n",
        "* Detén el entrenamiento en función de los resultados obtenidos en el conjunto de validación.\n",
        "* Utiliza una tasa de aprendizaje pequeña para que puedas ver cómo disminuye la pérdida en las distintas curvas.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314633ff-c8d9-4e77-88b8-0186d31c21ef",
      "metadata": {
        "id": "314633ff-c8d9-4e77-88b8-0186d31c21ef"
      },
      "source": [
        "### Before Starting! Some tips on GPU and Google Colab\n",
        "\n",
        "You will likely to run this code on Google Colab in order to use GPU ressources. If not, the code will take 60 times more to run!!  \n",
        "\n",
        "#### Stop disconnection\n",
        "\n",
        "Colab will disconnect you if you the training is too long. In order to prevent this, you can use this piece of code in the console of your browser:\n",
        "\n",
        "```java\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```\n",
        "\n",
        "See more [here](https://www.reddit.com/r/StableDiffusion/comments/xhhlp0/automatic1111_on_colab_how_to_keep_it_running/).\n",
        "\n",
        "#### Other account\n",
        "\n",
        "If you run out of available ressources, what you can do is changing of the account you are using in colab. You can create another gmail and switch to it, this should give you more GPU ressources.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66de14da-fc4a-47a3-aef2-8d031d943500",
      "metadata": {
        "id": "66de14da-fc4a-47a3-aef2-8d031d943500",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! wget https://users.dcc.uchile.cl/~vbarrier/MModal_Sentiment/Texts_Targeted.zip\n",
        "! unzip Texts_Targeted.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac593907-0f61-4536-b551-296b4988bf8b",
      "metadata": {
        "id": "ac593907-0f61-4536-b551-296b4988bf8b",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! wget https://users.dcc.uchile.cl/~vbarrier/MModal_Sentiment/Images.zip\n",
        "! unzip Images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2204e4c-c21c-42f7-bcc5-5e95942f335d",
      "metadata": {
        "id": "d2204e4c-c21c-42f7-bcc5-5e95942f335d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_data(fn):\n",
        "    df = pd.read_csv(fn, sep='\\t')[['tweet', 'label', 'image_id']]\n",
        "    X = df.tweet\n",
        "    y = df.label\n",
        "    path_image = df.image_id\n",
        "    return X, y, path_image\n",
        "\n",
        "split='train'\n",
        "X_train, y_train, path_image_train = read_data('MModal_Targeted_Sentiment_%s.tsv'%split)\n",
        "split='dev'\n",
        "X_dev, y_dev, path_image_dev = read_data('MModal_Targeted_Sentiment_%s.tsv'%split)\n",
        "split='test'\n",
        "X_test, y_test, path_image_test = read_data('MModal_Targeted_Sentiment_%s.tsv'%split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf22f35-0eb5-42fc-b4fe-09e7098d4ed1",
      "metadata": {
        "id": "6cf22f35-0eb5-42fc-b4fe-09e7098d4ed1"
      },
      "source": [
        "## Modelizacion\n",
        "\n",
        "**Metricas**: Para este tarea de clasificacion pueden utilizar el score F-1 para estimar las performancias de los modelos  \n",
        "\n",
        "**Multimodal Fusion**: The principle is to encode each modality and fuse them together.   \n",
        "\n",
        "**Text Encoding**: You need to encode the text into a vector. Implement:\n",
        "* a simple text encoding using a Bag-of-Word and a tf-idf, or an encoding using word2vec and aggregation using the arithmetic mean,\n",
        "* an encoding using RoBERTa by taking the `<s>` token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch, os, matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "AvxHQF_A2sDB"
      },
      "id": "AvxHQF_A2sDB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf encoding\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train_enc = le.transform(y_train)\n",
        "y_dev_enc = le.transform(y_dev)\n",
        "y_test_enc = le.transform(y_test)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
        "X_dev_tfidf = tfidf_vectorizer.transform(X_dev)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# RoBERTa encoding\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenize the tweets\n",
        "X_train_tokenized = tokenizer(\n",
        "    X_train.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "X_dev_tokenized = tokenizer(\n",
        "    X_dev.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "X_test_tokenized = tokenizer(\n",
        "    X_test.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "xLsHTvdgzrhn"
      },
      "id": "xLsHTvdgzrhn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Encoding**: You need to encode the image into a vector using an already pre-trained CNN that will create a representation of the image:\n",
        "* an EfficientNetB2 or EfficientNetB3 pre-trained over ImageNet\n",
        "* an EfficientNetB2 or EfficientNetB3 already fine-tuned over the Emotion Recognition dataset"
      ],
      "metadata": {
        "id": "8bTyUemTzp0T"
      },
      "id": "8bTyUemTzp0T"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "4_a0TOjJseqD"
      },
      "id": "4_a0TOjJseqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from efficientnet_pytorch import EfficientNet"
      ],
      "metadata": {
        "id": "sZ3MbHt3Ty2f"
      },
      "id": "sZ3MbHt3Ty2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained\n",
        "pre_trained = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "\n",
        "# This model was fine-tuned using the second homework\n",
        "fine_tuned = torch.load(\"/content/efficient_net_B3_fine_tuned.pt\")"
      ],
      "metadata": {
        "id": "yJIZNS7Z55Ww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a755c3-d27d-4569-cd34-3e32fb2b5909"
      },
      "id": "yJIZNS7Z55Ww",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-8ecde34f5206>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  fine_tuned = torch.load(\"/content/efficient_net_B3_fine_tuned.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78513458-1a40-4b38-a601-7801200ac855",
      "metadata": {
        "id": "78513458-1a40-4b38-a601-7801200ac855",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "#### Text Transformer Model\n",
        "\n",
        "Use a pre-trained model from the HuggingFace library and fine-tune it on the dataset.\n",
        "\n",
        "* For the data, you can use the classes `DataLoader` and `DataLoader` from `torch.utils.data`\n",
        "* For the modelization:\n",
        "  * You can use the classes `AutoTokenizer` and `AutoModel` from the `transformers` library\n",
        "  * Load the weights of the models already pre-trained, with the associated tokenizer!\n",
        "  * You can use a [classical roberta](https://huggingface.co/roberta-base) `'roberta-base'`\n",
        "  * You can also use another model from huggingface, like [a smaller model](https://huggingface.co/distilbert-base-uncased) such as `distilbert-base-uncased` (it is an example). Be careful if you do this! You will need to change the separation token of the tweets (which is `</s>` here)\n",
        "  \n",
        "#### Multimodal Model\n",
        "\n",
        "* For the fine-tuning (optimization):\n",
        "    * Make the loop in pytorch, **do not use** the class `Trainer` from the `transformers` library\n",
        "    * You can use classical loss functions like cross-entropy and the optimizer you want, `Adam` or `AdamW` for example, you can set the learning rate to `1e-5`, a batch size of `32` and `10` epochs. Be sure that you are converging well, by looking at the train and validation loss curves. These values worked for me!\n",
        "\n",
        "```python\n",
        "optimizer_params = {\n",
        "    'lr': 1e-5,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs':10,\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45c3234-f389-401c-96f3-542ea594daf6",
      "metadata": {
        "id": "c45c3234-f389-401c-96f3-542ea594daf6",
        "tags": []
      },
      "source": [
        "### Naive Fusions\n",
        "\n",
        "* Concatenate the audio and text representations: before naively fusing them with a MLP layer\n",
        "* Concatenate the penultimate layers (neural representations): the `<s>` vector for RoBERTa and the representation of the EfficientNet\n",
        "* Finally, when using the text encoder and the image encoder, run another experiment unfreezing the parameters of the whole network during fine-tuning\n",
        "* You can try using an EfficientNet already fine-tuned over the Emotion dataset and see the differences, it should help!  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e6a340-81aa-461c-ac6f-c08f945b9282",
      "metadata": {
        "id": "43e6a340-81aa-461c-ac6f-c08f945b9282",
        "tags": []
      },
      "source": [
        "## Utils:\n",
        "\n",
        "Code functions that can be utils for this task:\n",
        "* `MultimodalDataset(torch.utils.data.Dataset)` class that is modular with the different type of encodings from `torch.utils.data.Dataset`\n",
        "* `training()` function with the training loop\n",
        "* A class `MultimodalClassifier(nn.Module)` containing the network _per se_.\n",
        "\n",
        "Use data augmentation for the vision modality:\n",
        "```python\n",
        "rotation_value = 0.2\n",
        "# Image transformations for EfficientNet\n",
        "# For validation/test\n",
        "self.image_transform = transforms.Compose([\n",
        "        transforms.CenterCrop(self.image_size),\n",
        "        # transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "# Apply data augmentation at training\n",
        "self.image_transform_data_augmentation = transforms.Compose([\n",
        "    # transforms.RandomRotation(angle_rotation) # check if this works\n",
        "    transforms.RandomResizedCrop(self.image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(degrees=rotation_value * 360),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "```\n",
        "\n",
        "Also do not forget other regularizers like `Torch.nn.dropout` in the multimodal layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc4b96e-c4dd-46d6-9dc4-327685319b07",
      "metadata": {
        "id": "fbc4b96e-c4dd-46d6-9dc4-327685319b07"
      },
      "outputs": [],
      "source": [
        "from torch.nn import MultiheadAttention\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch import optim\n",
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13fc9ae-1172-4ee8-8c03-99e42d0f7f94",
      "metadata": {
        "id": "d13fc9ae-1172-4ee8-8c03-99e42d0f7f94"
      },
      "outputs": [],
      "source": [
        "def print_plots(train_losses, dev_losses, metric=\"Loss\"):\n",
        "    # Plotting training and validation of the given metric\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label=f\"Training {metric}\")\n",
        "    plt.plot(dev_losses, label=f\"Validation {metric}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(f\"{metric}\")\n",
        "    plt.title(f\"Training and Validation {metric} Curve\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 experiments using naive fusion"
      ],
      "metadata": {
        "id": "Sndqe-vMFlLG"
      },
      "id": "Sndqe-vMFlLG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without RoBERTa + frozen-EfficientNet"
      ],
      "metadata": {
        "id": "poJVPciuZb9W"
      },
      "id": "poJVPciuZb9W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, X_text_tfidf, image_paths, y_labels, augment=False):\n",
        "        self.X_text_tfidf = X_text_tfidf\n",
        "        self.image_paths = image_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.augment = augment\n",
        "        self.image_size = 224\n",
        "\n",
        "        rotation_value = 0.2\n",
        "        # Image transformations\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.image_transform_data_augmentation = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(self.image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomRotation(degrees=rotation_value * 360),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get text vector\n",
        "        text_vector = self.X_text_tfidf[idx].toarray().squeeze()\n",
        "        text_vector = torch.tensor(text_vector, dtype=torch.float32)\n",
        "\n",
        "        # Get image\n",
        "        image_path = self.image_paths.iloc[idx]\n",
        "        image_file = os.path.join(str(image_path))\n",
        "        image = Image.open(image_file).convert('RGB')\n",
        "        if self.augment:\n",
        "            image = self.image_transform_data_augmentation(image)\n",
        "        else:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        label = self.y_labels[idx]\n",
        "        return text_vector, image, label"
      ],
      "metadata": {
        "id": "Htlz-vooSHlw"
      },
      "id": "Htlz-vooSHlw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = MultimodalDataset(X_train_tfidf, path_image_train, y_train_enc, augment=True)\n",
        "dev_dataset = MultimodalDataset(X_dev_tfidf, path_image_dev, y_dev_enc)\n",
        "test_dataset = MultimodalDataset(X_test_tfidf, path_image_test, y_test_enc)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "4T5SuiUbSMUM"
      },
      "id": "4T5SuiUbSMUM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        # Image encoder\n",
        "        self.image_model = pre_trained\n",
        "        # Freeze image model parameters\n",
        "        for param in self.image_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Get the number of features from image model\n",
        "        image_feature_size = self.image_model._fc.in_features  # 1408 for EfficientNet-B2\n",
        "\n",
        "        # Fusion and classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(text_feature_size + image_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_vector, image):\n",
        "        # Image features\n",
        "        image_features = self.image_model.extract_features(image)\n",
        "        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1))\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Fuse features\n",
        "        fused_features = torch.cat((text_vector, image_features), dim=1)\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(fused_features)\n",
        "        return output\n",
        "\n",
        "# Initialize model\n",
        "text_feature_size = X_train_tfidf.shape[1]  # 5000\n",
        "model = MultimodalClassifier(text_feature_size, num_classes)"
      ],
      "metadata": {
        "id": "8GW9NQ2kSQ-I"
      },
      "id": "8GW9NQ2kSQ-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def training(model, train_loader, dev_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_f1_scores = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for text_vector, images, labels in train_loader:\n",
        "            text_vector = text_vector.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_vector, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * text_vector.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text_vector, images, labels in dev_loader:\n",
        "                text_vector = text_vector.to(device)\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(text_vector, images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * text_vector.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(dev_loader.dataset)\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_f1_scores.append(train_f1)\n",
        "        val_f1_scores.append(val_f1)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Train Loss: {avg_train_loss:.4f}, Train F1: {train_f1:.4f}, '\n",
        "              f'Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    return model, train_losses, val_losses, train_f1_scores, val_f1_scores"
      ],
      "metadata": {
        "id": "ahMksZxTSXE1"
      },
      "id": "ahMksZxTSXE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, val_losses, train_f1_scores, val_f1_scores = training(model, train_loader, dev_loader,\n",
        "                                                                           criterion, optimizer, num_epochs, device)\n",
        "\n",
        "print_plots(train_losses, val_losses)\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text_vector, images, labels in test_loader:\n",
        "        text_vector = text_vector.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(text_vector, images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "print(f'Test F1-score: {test_f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "kMzf_6aySfU0",
        "outputId": "665b963b-2b52-42a0-ebfe-9dbe04b8b96f"
      },
      "id": "kMzf_6aySfU0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.0466, Train F1: 0.3898, Val Loss: 1.0518, Val F1: 0.3586\n",
            "Epoch 2/10, Train Loss: 0.9947, Train F1: 0.3696, Val Loss: 1.0315, Val F1: 0.3609\n",
            "Epoch 3/10, Train Loss: 0.9843, Train F1: 0.3694, Val Loss: 1.0224, Val F1: 0.3698\n",
            "Epoch 4/10, Train Loss: 0.9793, Train F1: 0.3711, Val Loss: 1.0166, Val F1: 0.3712\n",
            "Epoch 5/10, Train Loss: 0.9724, Train F1: 0.3761, Val Loss: 1.0116, Val F1: 0.3806\n",
            "Epoch 6/10, Train Loss: 0.9677, Train F1: 0.3915, Val Loss: 1.0083, Val F1: 0.3900\n",
            "Epoch 7/10, Train Loss: 0.9662, Train F1: 0.3991, Val Loss: 1.0049, Val F1: 0.3913\n",
            "Epoch 8/10, Train Loss: 0.9601, Train F1: 0.3959, Val Loss: 1.0026, Val F1: 0.4040\n",
            "Epoch 9/10, Train Loss: 0.9593, Train F1: 0.4154, Val Loss: 0.9998, Val F1: 0.4089\n",
            "Epoch 10/10, Train Loss: 0.9575, Train F1: 0.4160, Val Loss: 0.9972, Val F1: 0.4068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-e1a58ed378fa>:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1-score: 0.4201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### frozen-RoBERTa + frozen-EfficientNet"
      ],
      "metadata": {
        "id": "dC85jZAgHxjU"
      },
      "id": "dC85jZAgHxjU"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, text_inputs, image_paths, y_labels, augment=False):\n",
        "        self.text_inputs = text_inputs  # Tokenized RoBERTa inputs\n",
        "        self.image_paths = image_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.augment = augment\n",
        "        self.image_size = 224\n",
        "\n",
        "        rotation_value = 0.2\n",
        "        # Image transformations\n",
        "        self.image_transform = transforms.Compose([\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.image_transform_data_augmentation = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(self.image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomRotation(degrees=rotation_value * 360),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Text inputs (tokenized by RoBERTa)\n",
        "        text_input = {key: val[idx] for key, val in self.text_inputs.items()}\n",
        "\n",
        "        # Get image\n",
        "        image_path = self.image_paths.iloc[idx]\n",
        "        image_file = os.path.join(str(image_path))\n",
        "        image = Image.open(image_file).convert('RGB')\n",
        "        if self.augment:\n",
        "            image = self.image_transform_data_augmentation(image)\n",
        "        else:\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "        # Label\n",
        "        label = self.y_labels[idx]\n",
        "        return text_input, image, label"
      ],
      "metadata": {
        "id": "a4zIU4MbXZ0g"
      },
      "id": "a4zIU4MbXZ0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, roberta_model_name='roberta-base', num_classes=10):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # Load frozen RoBERTa\n",
        "        self.text_encoder = AutoModel.from_pretrained(roberta_model_name)\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Load frozen EfficientNetB3\n",
        "        self.image_encoder = pre_trained\n",
        "        for param in self.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Fusion and classifier\n",
        "        text_feature_size = self.text_encoder.config.hidden_size  # 768 for roberta-base\n",
        "        image_feature_size = 1536  # EfficientNetB3 output size\n",
        "        fused_size = text_feature_size + image_feature_size\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fused_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_inputs, images):\n",
        "        # Text features\n",
        "        text_outputs = self.text_encoder(**text_inputs)\n",
        "        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use <s> token\n",
        "\n",
        "        # Image features\n",
        "        image_features = self.image_encoder(images)\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Fuse features\n",
        "        fused_features = torch.cat((text_features, image_features), dim=1)\n",
        "\n",
        "        # Classify\n",
        "        outputs = self.classifier(fused_features)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "r4yeufKwYBux"
      },
      "id": "r4yeufKwYBux",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model, train_loader, dev_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_f1_scores = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for text_inputs, images, labels in train_loader:\n",
        "            # Move inputs to device\n",
        "            text_inputs = {key: val.to(device) for key, val in text_inputs.items()}\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_inputs, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text_inputs, images, labels in dev_loader:\n",
        "                text_inputs = {key: val.to(device) for key, val in text_inputs.items()}\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(text_inputs, images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * labels.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(dev_loader.dataset)\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_f1_scores.append(train_f1)\n",
        "        val_f1_scores.append(val_f1)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Train Loss: {avg_train_loss:.4f}, Train F1: {train_f1:.4f}, '\n",
        "              f'Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return model, train_losses, val_losses, train_f1_scores, val_f1_scores"
      ],
      "metadata": {
        "id": "IXwlOeIiYIEC"
      },
      "id": "IXwlOeIiYIEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text_inputs, images, labels in data_loader:\n",
        "            # Move inputs to device\n",
        "            text_inputs = {key: val.to(device) for key, val in text_inputs.items()}\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Make predictions\n",
        "            outputs = model(text_inputs, images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "    report = classification_report(test_labels, test_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test F1-score: {f1:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return accuracy, f1, report"
      ],
      "metadata": {
        "id": "g1uMlQBEY4bA"
      },
      "id": "g1uMlQBEY4bA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset and dataloaders\n",
        "train_dataset = MultimodalDataset(X_train_tokenized, path_image_train, y_train_enc, augment=True)\n",
        "dev_dataset = MultimodalDataset(X_dev_tokenized, path_image_dev, y_dev_enc)\n",
        "test_dataset = MultimodalDataset(X_test_tokenized, path_image_test, y_test_enc)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Initialize model\n",
        "num_classes = len(set(y_train_enc))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultimodalClassifier(num_classes=num_classes)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, val_losses, train_f1_scores, val_f1_scores = training(\n",
        "    model, train_loader, dev_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "print_plots(train_losses, val_losses)\n",
        "\n",
        "# Evaluate on the test set\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "tRI8PKdnYXfk"
      },
      "id": "tRI8PKdnYXfk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### frozen-RoBERTa + frozen-EfficientNet-emo"
      ],
      "metadata": {
        "id": "sqTSDp35JWIP"
      },
      "id": "sqTSDp35JWIP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the multimodal classifier\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        # Image encoder\n",
        "        self.image_model = fine_tuned\n",
        "        for param in self.image_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        image_feature_size = self.image_model._fc.in_features\n",
        "\n",
        "        # Fusion and classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(text_feature_size + image_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_vector, image):\n",
        "        # Extract image features\n",
        "        image_features = self.image_model.extract_features(image)\n",
        "        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1))\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Fuse features\n",
        "        fused_features = torch.cat((text_vector, image_features), dim=1)\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XqnKV3OAZdUd"
      },
      "id": "XqnKV3OAZdUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "text_feature_size = X_train_tfidf.shape[1]\n",
        "num_classes = len(set(y_train_enc))\n",
        "model = MultimodalClassifier(text_feature_size, num_classes)"
      ],
      "metadata": {
        "id": "3U3ammR8aDmx"
      },
      "id": "3U3ammR8aDmx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, val_losses, train_f1_scores, val_f1_scores = training(model, train_loader, dev_loader,\n",
        "                                                                           criterion, optimizer, num_epochs, device)\n",
        "\n",
        "print_plots(train_losses, val_losses)\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "lb3rxtQtaEDK"
      },
      "id": "lb3rxtQtaEDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoBERTa + EfficientNet"
      ],
      "metadata": {
        "id": "aedjsU-ELZOm"
      },
      "id": "aedjsU-ELZOm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the multimodal classifier\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        # Image encoder\n",
        "        self.image_model = pre_trained\n",
        "        image_feature_size = self.image_model._fc.in_features\n",
        "\n",
        "        # Fusion and classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(text_feature_size + image_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_vector, image):\n",
        "        # Extract image features\n",
        "        image_features = self.image_model.extract_features(image)\n",
        "        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1))\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Fuse features\n",
        "        fused_features = torch.cat((text_vector, image_features), dim=1)\n",
        "\n",
        "        # Classify\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "FsJSx_vNar7b"
      },
      "id": "FsJSx_vNar7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "text_feature_size = X_train_tfidf.shape[1]  # Assuming 5000\n",
        "num_classes = len(set(y_train_enc))\n",
        "model = MultimodalClassifier(text_feature_size, num_classes)"
      ],
      "metadata": {
        "id": "f1bRqzvPa6NS"
      },
      "id": "f1bRqzvPa6NS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, val_losses, train_f1_scores, val_f1_scores = training(model, train_loader, dev_loader,\n",
        "                                                                           criterion, optimizer, num_epochs, device)\n",
        "\n",
        "print_plots(train_losses, val_losses)\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "ZUmEWXA6a77_"
      },
      "id": "ZUmEWXA6a77_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "421bd0a8-6d91-4295-be90-c1d4009b4a36",
      "metadata": {
        "id": "421bd0a8-6d91-4295-be90-c1d4009b4a36"
      },
      "source": [
        "### Attention Fusions\n",
        "\n",
        "Implement attention mechanism on the different modalities in order to know its importance for the final decision while fusing.\n",
        "\n",
        "* Self-attention\n",
        "* Cross-modal Attention\n",
        "\n",
        "Use the `MultiheadAttention` function from pytorch, with 2 heads, or a concatenation of the vector weighted with attention and the initial vector.  \n",
        "\n",
        "Here is an example for the cross-modal attention, in the forward:\n",
        "```python\n",
        "att_text_embedding, _ = self.text_to_image_attention(text_embedding, image_embedding, image_embedding)\n",
        "text_embedding = torch.cat((text_embedding, att_text_embedding), dim=1)\n",
        "text_embedding = torch.relu(self.text_self_attention_fc(text_embedding))\n",
        "\n",
        "att_image_embedding, _ = self.image_to_text_attention(image_embedding, text_embedding, text_embedding)\n",
        "image_embedding = torch.cat((image_embedding, att_image_embedding), dim=1)\n",
        "image_embedding = torch.relu(self.text_self_attention_fc(image_embedding))\n",
        "```                    \n",
        "\n",
        "For these experiments, you should unfreeze a part of the whole network, using the `unfreeze_params()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9d74a2-dc15-4e84-b05f-a8be9e977194",
      "metadata": {
        "id": "cd9d74a2-dc15-4e84-b05f-a8be9e977194"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.efficientnet import MBConv\n",
        "from torchvision.ops.misc import SqueezeExcitation\n",
        "\n",
        "def unfreeze_params(image_encoder, first_block_to_uf = 7):\n",
        "    \"\"\"\n",
        "    Works at least for EfficientB3. Will unfreeze from layer `first_block_to_uf` (the higher this number the less layers will be unfrozen)\n",
        "    \"\"\"\n",
        "    if first_block_to_uf > 0:\n",
        "        # Freeze all layers first\n",
        "        for param in image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze specified block and subsequent layers\n",
        "        bool_start_unfreezing = False\n",
        "        for name, layer in list(image_encoder.named_children()):\n",
        "            if isinstance(layer, nn.Sequential):\n",
        "                for sub_name, sub_layer in list(layer.named_children()):\n",
        "                    if int(sub_name) >= first_block_to_uf or name == 'classifier':\n",
        "                        if name == 'classifier':\n",
        "                            for param in sub_layer.parameters():\n",
        "                                param.requires_grad = True\n",
        "                        else:\n",
        "                            print(f\"Unfreeze layer {sub_name}\")\n",
        "                            if isinstance(sub_layer, nn.Sequential):\n",
        "                                for sub_sub_layer in sub_layer:\n",
        "                                    if isinstance(sub_sub_layer, MBConv):\n",
        "                                        seq_mbconv = list(sub_sub_layer.children())[0]\n",
        "                                        for sub_seq_mbconv in seq_mbconv:\n",
        "                                            # is squeeze, change\n",
        "                                            if not isinstance(sub_seq_mbconv, SqueezeExcitation):\n",
        "                                                for sub_sub_seq_mbconv in sub_seq_mbconv:\n",
        "                                                    if not isinstance(sub_sub_seq_mbconv, nn.BatchNorm2d):\n",
        "                                                        for param in sub_sub_seq_mbconv.parameters():\n",
        "                                                            param.requires_grad = True\n",
        "                                            else:\n",
        "                                                for param in sub_seq_mbconv.parameters():\n",
        "                                                    param.requires_grad = True\n",
        "\n",
        "                                    else:\n",
        "                                        if not isinstance(sub_sub_layer, nn.BatchNorm2d):\n",
        "                                            for param in sub_sub_layer.parameters():\n",
        "                                                    param.requires_grad = True\n",
        "                            for sub_sub_layer in sub_layer:\n",
        "                                if not isinstance(sub_sub_layer, nn.BatchNorm2d):\n",
        "                                    for param in sub_sub_layer.parameters():\n",
        "                                        param.requires_grad = True\n",
        "\n",
        "    return image_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Self-attention"
      ],
      "metadata": {
        "id": "dyMfFcY3cuV6"
      },
      "id": "dyMfFcY3cuV6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ddc4c8-94ef-4107-8ab3-b0a538f1189d",
      "metadata": {
        "id": "60ddc4c8-94ef-4107-8ab3-b0a538f1189d"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionMultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes, pre_trained_model, unfreeze_block=7):\n",
        "        super(SelfAttentionMultimodalClassifier, self).__init__()\n",
        "        # Image encoder\n",
        "        self.image_model = unfreeze_params(pre_trained_model, first_block_to_uf=unfreeze_block)\n",
        "        image_feature_size = self.image_model._fc.in_features\n",
        "\n",
        "        # Self-attention for text and image\n",
        "        self.text_self_attention = MultiheadAttention(embed_dim=text_feature_size, num_heads=2, batch_first=True)\n",
        "        self.image_self_attention = MultiheadAttention(embed_dim=image_feature_size, num_heads=2, batch_first=True)\n",
        "\n",
        "        # Fusion and classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(text_feature_size + image_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_vector, image):\n",
        "        # Extract image features\n",
        "        image_features = self.image_model.extract_features(image)\n",
        "        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1))\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Self-attention for text\n",
        "        text_embedding, _ = self.text_self_attention(text_vector.unsqueeze(1), text_vector.unsqueeze(1), text_vector.unsqueeze(1))\n",
        "        text_embedding = text_embedding.squeeze(1)\n",
        "\n",
        "        # Self-attention for image\n",
        "        image_embedding, _ = self.image_self_attention(image_features.unsqueeze(1), image_features.unsqueeze(1), image_features.unsqueeze(1))\n",
        "        image_embedding = image_embedding.squeeze(1)\n",
        "\n",
        "        # Fuse features and classify\n",
        "        fused_features = torch.cat((text_embedding, image_embedding), dim=1)\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "text_feature_size = X_train_tfidf.shape[1]\n",
        "num_classes = len(set(y_train_enc))\n",
        "\n",
        "self_attention_model = SelfAttentionMultimodalClassifier(text_feature_size, num_classes, pre_trained, unfreeze_block=7)\n",
        "optimizer_self = torch.optim.Adam(self_attention_model.parameters(), lr=1e-5)\n",
        "\n",
        "# Train Self-Attention Model\n",
        "self_attention_model, self_train_losses, self_val_losses, self_train_f1, self_val_f1 = training(\n",
        "    self_attention_model, train_loader, dev_loader, criterion, optimizer_self, num_epochs, device)\n",
        "\n",
        "# Evaluate\n",
        "evaluate(self_attention_model, test_loader, device)"
      ],
      "metadata": {
        "id": "B0PZB6E7c4ql"
      },
      "id": "B0PZB6E7c4ql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-modal attention"
      ],
      "metadata": {
        "id": "7EPrSX3VcxiT"
      },
      "id": "7EPrSX3VcxiT"
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossModalAttentionMultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes, pre_trained_model, unfreeze_block=7):\n",
        "        super(CrossModalAttentionMultimodalClassifier, self).__init__()\n",
        "        # Image encoder\n",
        "        self.image_model = unfreeze_params(pre_trained_model, first_block_to_uf=unfreeze_block)\n",
        "        image_feature_size = self.image_model._fc.in_features\n",
        "\n",
        "        # Cross-modal attention\n",
        "        self.text_to_image_attention = MultiheadAttention(embed_dim=image_feature_size, num_heads=2, batch_first=True)\n",
        "        self.image_to_text_attention = MultiheadAttention(embed_dim=text_feature_size, num_heads=2, batch_first=True)\n",
        "\n",
        "        # Fusion and classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(text_feature_size + image_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_vector, image):\n",
        "        # Extract image features\n",
        "        image_features = self.image_model.extract_features(image)\n",
        "        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1))\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "\n",
        "        # Cross-modal attention: text attends to image\n",
        "        att_text_embedding, _ = self.text_to_image_attention(text_vector.unsqueeze(1), image_features.unsqueeze(1), image_features.unsqueeze(1))\n",
        "        text_embedding = att_text_embedding.squeeze(1)\n",
        "\n",
        "        # Cross-modal attention: image attends to text\n",
        "        att_image_embedding, _ = self.image_to_text_attention(image_features.unsqueeze(1), text_vector.unsqueeze(1), text_vector.unsqueeze(1))\n",
        "        image_embedding = att_image_embedding.squeeze(1)\n",
        "\n",
        "        # Fuse features and classify\n",
        "        fused_features = torch.cat((text_embedding, image_embedding), dim=1)\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "E9A4hjQ3c1nA"
      },
      "id": "E9A4hjQ3c1nA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_modal_model = CrossModalAttentionMultimodalClassifier(text_feature_size, num_classes, pre_trained, unfreeze_block=7)\n",
        "optimizer_cross = torch.optim.Adam(cross_modal_model.parameters(), lr=1e-5)\n",
        "\n",
        "# Train Cross-Modal Attention Model\n",
        "cross_modal_model, cross_train_losses, cross_val_losses, cross_train_f1, cross_val_f1 = training(\n",
        "    cross_modal_model, train_loader, dev_loader, criterion, optimizer_cross, num_epochs, device)\n",
        "\n",
        "# Evaluate\n",
        "evaluate(cross_modal_model, test_loader, device)"
      ],
      "metadata": {
        "id": "eDgcGiIUc6wG"
      },
      "id": "eDgcGiIUc6wG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "527a8c7c-5668-4a41-95e4-2a8ecb3f0bfb",
      "metadata": {
        "id": "527a8c7c-5668-4a41-95e4-2a8ecb3f0bfb",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Preguntas\n",
        "\n",
        "* ¿Cuál es el impacto de fine-tunear el CNN sobre una tarea conexa? (tipo Emociones)\n",
        "* ¿Cuál es el impacto de descongelar los pesos?  \n",
        "* ¿Funcionan bien las atenciones más complejas?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f1481a-499e-4f71-b911-6254d94bb3b8",
      "metadata": {
        "id": "a0f1481a-499e-4f71-b911-6254d94bb3b8",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Respuestas\n",
        "\n",
        "- Fine-tunear una CNN en una tarea conexa mejora la extracción de características al alinear el modelo con aspectos específicos del dominio, como las expresiones emocionales en imágenes. Esta adaptación acelera la convergencia, mejora el rendimiento y captura patrones sutiles relevantes para el dominio objetivo.\n",
        "\n",
        "- Descongelar los pesos permite que el modelo refine las características preentrenadas para la tarea objetivo, mejorando la adaptabilidad. Aunque esto mejora el rendimiento cuando hay suficientes datos de entrenamiento disponibles, aumenta los requisitos computacionales y el riesgo de sobreajuste.\n",
        "\n",
        "- Los mecanismos de atención complejos mejoran las interacciones multimodales al alinear mejor las modalidades y capturar relaciones más profundas. A menudo generan mejoras en el rendimiento en tareas que requieren una fusión detallada de características. Sin embargo, su costo computacional y el riesgo de sobreajuste hacen que sean difíciles de entrenar, especialmente en conjuntos de datos pequeños."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8435c318-2f94-4862-b98b-cdee4c27fd5a",
      "metadata": {
        "id": "8435c318-2f94-4862-b98b-cdee4c27fd5a",
        "tags": []
      },
      "source": [
        "## Fine-tuning Pre-trained Multimodal Models\n",
        "\n",
        "Use a pre-trained multimodal model from the Hugging Face library and fine-tune it on a dataset of paired text and image data.\n",
        "\n",
        "* **Data Preparation**  \n",
        "    * Use `torch.utils.data.DataLoader` to load a dataset with both text and image modalities, taking as input a `torch.utils.data.Dataset` object.\n",
        "    * Prepare the text and images, ensuring that they are appropriately tokenized and transformed:\n",
        "      * For text, use `Tokenizer` classes associated with your chosen model.\n",
        "      * For images, apply standard transformations (resize, normalization) compatible with your model.\n",
        "\n",
        "* **Model Selection**  \n",
        "   Load the model and associated tokenizer (and image processor, if applicable) with pre-trained weights. Here are examples of pre-trained models available on Hugging Face:\n",
        "\n",
        "### Without Instructions:\n",
        "   * **[ViLT (Vision-and-Language Transformer)](https://huggingface.co/docs/transformers/model_doc/vilt)** - `'dandelin/vilt-b32-mlm'`\n",
        "   * **[LXMERT (Learning Cross-Modality Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/model_doc/lxmert)** - `'unc-nlp/lxmert-base-uncased'`\n",
        "\n",
        "* **Fine-tuning (Optimization)**  \n",
        "    * Fine-tune a simple model using the output token (such as ViLT, using `outputs.pooler_output`; be careful of `max_length` when tokenizing) using a training loop in PyTorch without using the `Trainer` class:\n",
        "      * Use an optimizer like `AdamW` and a relevant loss function (e.g., cross-entropy for classification tasks).\n",
        "      * Set up a training loop where you load a batch of paired text and images, forward it through the model, calculate the loss, and perform backpropagation.\n",
        "      \n",
        "### With Instructions:\n",
        "   * **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** - `'Salesforce/blip2-opt-2.7b'`\n",
        "   * **[LLaVA (Large Language and Vision Assistant)](https://huggingface.co/docs/transformers/model_doc/llava)** - `'llava/vicuna-7b-v1.5'`\n",
        "   * **[BLIP3/xGen-MM](https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-r-v1)** - `'Salesforce/xgen-mm-phi3-mini-instruct-r-v1'`\n",
        "      \n",
        "* **In-Context-Learning / Zero-shot Learning**\n",
        "    * Use In-Context/Zero-shot Learning with a model fine-tuned with instructions (like [in this example](https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-r-v1/blob/main/demo.ipynb))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b836cff-d4c6-45c0-b4c2-71b376832c8b",
      "metadata": {
        "tags": [],
        "id": "0b836cff-d4c6-45c0-b4c2-71b376832c8b"
      },
      "source": [
        "You can use a `collate_fn` in order to process batch by batch, when passing the `Dataset` object to `DataLoader`.\n",
        "\n",
        "```python\n",
        "def collate_fn(batch):\n",
        "    texts, images, labels = zip(*batch)\n",
        "    \n",
        "    inputs = processor(images=images, text=list(texts), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Add labels to the inputs\n",
        "    inputs[\"label\"] = torch.stack(labels)\n",
        "    \n",
        "    return inputs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With instructions"
      ],
      "metadata": {
        "id": "4m2_Ns4yjinj"
      },
      "id": "4m2_Ns4yjinj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655e502f-7979-4ade-8f79-4e91e62d9319",
      "metadata": {
        "id": "655e502f-7979-4ade-8f79-4e91e62d9319"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_paths, labels, processor):\n",
        "        self.text_data = text_data\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return text, image, torch.tensor(label)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, images, labels = zip(*batch)\n",
        "    inputs = processor(images=list(images), text=list(texts), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs[\"labels\"] = torch.stack(labels)\n",
        "    return inputs\n",
        "\n",
        "# Example Dataset Preparation\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "text_data = [\"A caption for image 1\", \"A caption for image 2\", ...]\n",
        "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
        "labels = [0, 1, ...]  # Example labels for classification\n",
        "\n",
        "dataset = MultimodalDataset(text_data, image_paths, labels, processor)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltForSequenceClassification\n",
        "\n",
        "# Load pre-trained ViLT model for classification\n",
        "model = ViltForSequenceClassification.from_pretrained(\"dandelin/vilt-b32-mlm\", num_labels=2)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "e4rcePUMlPMz"
      },
      "id": "e4rcePUMlPMz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loop = tqdm(dataloader, leave=True)\n",
        "    for batch in loop:\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch {epoch+1}\")\n",
        "        loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(dataloader):.4f}, Accuracy: {correct/total:.4f}\")"
      ],
      "metadata": {
        "id": "aMu6TkoSlPxg"
      },
      "id": "aMu6TkoSlPxg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "KjNigP4plUtv"
      },
      "id": "KjNigP4plUtv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without instructions"
      ],
      "metadata": {
        "id": "nU2FHCBWjkq9"
      },
      "id": "nU2FHCBWjkq9"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, text_data, image_paths, labels, processor):\n",
        "        self.text_data = text_data\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return text, image, torch.tensor(label)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, images, labels = zip(*batch)\n",
        "    inputs = processor(images=list(images), text=list(texts), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs[\"labels\"] = torch.stack(labels)\n",
        "    return inputs\n",
        "\n",
        "# Example Dataset Preparation\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\")\n",
        "text_data = [\"What is the emotion in this image?\", \"Describe the sentiment of the image.\", ...]\n",
        "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
        "labels = [0, 1, ...]  # Example labels for classification\n",
        "\n",
        "dataset = MultimodalDataset(text_data, image_paths, labels, processor)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "FsK273CkjmYO"
      },
      "id": "FsK273CkjmYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipForConditionalGeneration\n",
        "\n",
        "# Load pre-trained BLIP3 model fine-tuned with instructions for in-context learning\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\", num_labels=2)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "CtC_zBTvnRBQ"
      },
      "id": "CtC_zBTvnRBQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loop = tqdm(dataloader, leave=True)\n",
        "    for batch in loop:\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass with instruction-based in-context learning\n",
        "        outputs = model(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch {epoch+1}\")\n",
        "        loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(dataloader):.4f}, Accuracy: {correct/total:.4f}\")"
      ],
      "metadata": {
        "id": "KcOGQOYDnXos"
      },
      "id": "KcOGQOYDnXos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass with zero-shot learning\n",
        "        outputs = model(**inputs)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "zN8pOkMvndW9"
      },
      "id": "zN8pOkMvndW9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83179e2f-9ee3-411c-b403-d4e5e4f39b34",
      "metadata": {
        "id": "83179e2f-9ee3-411c-b403-d4e5e4f39b34"
      },
      "source": [
        "### Preguntas\n",
        "\n",
        "* How are the performances using multimodaly pre-trained model, compared with other ones?  \n",
        "* What is the interest of Instructed Models? What could you see with the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ca6b77-fb58-4993-bce7-bf62a1aac887",
      "metadata": {
        "id": "c4ca6b77-fb58-4993-bce7-bf62a1aac887"
      },
      "source": [
        "#### Respuestas\n",
        "\n",
        "-\n",
        "\n",
        "-"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "Sndqe-vMFlLG",
        "poJVPciuZb9W",
        "dC85jZAgHxjU",
        "sqTSDp35JWIP",
        "aedjsU-ELZOm",
        "dyMfFcY3cuV6",
        "7EPrSX3VcxiT"
      ]
    },
    "kernelspec": {
      "display_name": "Python [deeplearning_env]",
      "language": "python",
      "name": "conda-env-deeplearning_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}